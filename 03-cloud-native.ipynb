{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8072d2d0",
   "metadata": {},
   "source": [
    "# Cloud-native Geospatial\n",
    "\n",
    "The cloud and geospatial data analysis go together very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0931cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adlfs\n",
    "import planetary_computer\n",
    "import ipyleaflet\n",
    "import requests\n",
    "import shapely.geometry\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881124fd-9da6-4a64-ae74-bd358a86ca5e",
   "metadata": {},
   "source": [
    "## Cloud-native Principals\n",
    "\n",
    "The Planetary Computer implements many cloud-native concepts. Here, we'll just list some of them. Later on, we'll apply them.\n",
    "\n",
    "1. You have *direct* access to *all* of the data\n",
    "    - You have access to PBs of data\n",
    "    - Data assets are hosted in the highly scalable Azure Blob Storage\n",
    "    - You have direct access to the files, using plain HTTPs or Azure Blob Storage APIs. This means you can open the files using any tool that can speak HTTP\n",
    "2. Cloud-native formats\n",
    "    - Wherever possible, we use cloud-native / friendly file formats. We'll see examples using COG, Zarr, (geo)parquet, and COPC\n",
    "    - Efficient access to metadata (don't need to read the whole file to understand its contents)\n",
    "    - Efficient access to subsets of the data\n",
    "3. Compute is next to the data\n",
    "    - All of our files are in the West Europe Azure data region. For best performance, compute should be in that same data center.\n",
    "4. Ability to scale\n",
    "    - Azure makes it easy to get lots of compute\n",
    "\n",
    "\n",
    "### Compute â†’ Data\n",
    "\n",
    "Putting the compute next to the data can be crucial for performance. Let's consider the simple task of reading the metadata from a COG file with `gdalinfo`.\n",
    "\n",
    "Running this command from my laptop in Des Moines, IA, we spend a *lot* of time waiting:\n",
    "\n",
    "```console\n",
    "$ time gdalinfo /vsicurl/https://naipeuwest.blob.core.windows.net/naip/v002/ia/2019/ia_60cm_2019/42091/m_4209150_sw_15_060_20190828.tif > /dev/null\n",
    "real    0m7.158s\n",
    "user    0m0.195s\n",
    "sys     0m0.032s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa6826-f0a1-4ce4-b93e-b684b9d72696",
   "metadata": {},
   "source": [
    "Running that from this Jupyter kernel, which is in the same Azure data center as the dataset, things look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe036d-89d9-4717-82f7-b64310121765",
   "metadata": {},
   "outputs": [],
   "source": [
    "!time gdalinfo /vsicurl/https://naipeuwest.blob.core.windows.net/naip/v002/ia/2019/ia_60cm_2019/42091/m_4209150_sw_15_060_20190828.tif > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c1a54-9c8e-4566-8970-22e310a32dbb",
   "metadata": {},
   "source": [
    "So a nice 35x speedup!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97a5e6-a7e8-4c4b-adb3-d83d208275d1",
   "metadata": {},
   "source": [
    "## STAC\n",
    "\n",
    "Having access to the data is great, but it's not enough. For example, how would you find all the Sentinel-2 images over Wyoming for July 2021? Consider what we'd do if we just had files in blob storage. We'll use `adlfs` to list some folders, to try to figure out the naming convention (we could also read the docs, but where's the fun in that?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04472f-f054-4460-b342-159363621978",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"], bbox=wyoming_bbox, datetime=\"2021-07-01/2021-07-31\"\n",
    ")\n",
    "items = search.get_all_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13343f76-fa84-424f-8a99-ab8ecbcbfed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8fe313-be32-4fa8-8483-1e1883db1b3e",
   "metadata": {},
   "source": [
    "Even better: STAC is a standard. It isn't specific to Sentinel-2, or even remote sensing data. Landsat Collection 2 Level-2, which uses a completely different folder structure in blob storage, can be searched by just chagning the collection ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182640ee-2453-4507-92f7-3bee685fb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=wyoming_bbox,\n",
    "    datetime=\"2021-07-01/2021-07-31\",\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    ")\n",
    "%time items = search.get_all_items()\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77eefd7-ab77-491b-bdbb-ab75ef3d969b",
   "metadata": {},
   "source": [
    "## Data APIs\n",
    "\n",
    "The Planetary Computer also provides a data API, based on [TiTiler](https://developmentseed.org/titiler/), which provides endpoints for some common geospatial analysis routines. This can be a nice alternative to setting up your own compute in Azure if you're doing something basic, like putting an image on a Map (or even more advanced things like mosaicing many images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2ce58-d7d0-4332-9bc1-c207d59750c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import shapely.geometry\n",
    "import requests\n",
    "import ipyleaflet\n",
    "\n",
    "catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=[-111.0717, 41.0296, -103.9965, 45.02695],\n",
    "    datetime=\"2021-07-01/2021-07-31\",\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    ")\n",
    "%time items = search.get_all_items()\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f1a54-f94c-4d1d-86f1-8c156d25997c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item = items[1]\n",
    "\n",
    "(tiles_url,) = requests.get(item.assets[\"tilejson\"].href).json()[\"tiles\"]\n",
    "center = shapely.geometry.shape(item.geometry).centroid.bounds[1::-1]\n",
    "\n",
    "m = ipyleaflet.Map(\n",
    "    center=center, controls=[ipyleaflet.FullScreenControl()],\n",
    ")\n",
    "m.add_layer(ipyleaflet.TileLayer(url=tiles_url))\n",
    "m.scroll_wheel_zoom = True\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f03968-cb4f-4ce3-9dd3-5218f4cca2d9",
   "metadata": {},
   "source": [
    "Fun fact: the STAC and Data APIs power our [explorer](https://planetarycomputer.microsoft.com/explore?c=118.8189%2C37.4070&z=11.00)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a5a78-2232-4ed5-bf87-c03570a00c88",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Azure offers *many* ways to do compute. The [pangeo](https://pangeo.io/) community has done a ton of work on scalable JupyterHub deployments. This JupyterHub is deployed with [Dask Gateway](https://gateway.dask.org/)\n",
    "\n",
    "<img src=\"https://gateway.dask.org/_images/architecture.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b9986-7edb-49aa-9de5-35a66d0f0736",
   "metadata": {},
   "source": [
    "As a user, you just need to request a cluster and scale it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f317b63-f036-4e99-b306-9975a0328358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_gateway\n",
    "\n",
    "cluster = dask_gateway.GatewayCluster()\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849a117-22c4-4870-bbde-74754992ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa22fd2-4a53-4c09-b74a-5f470487c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe\n",
    "\n",
    "ts = dask.dataframe.demo.make_timeseries()\n",
    "df = ts.groupby(\"name\").agg([\"mean\", \"count\", \"sum\"]).compute()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94ac81-7317-4574-a376-dfecdc30bc8e",
   "metadata": {},
   "source": [
    "In the background the Gateway service is talking to [Azure Kubernetes Service](https://docs.microsoft.com/en-us/azure/aks/) to start up scheduler and worker pods that will form our Dask Cluster. Kubernetes is in turn asking Azure for more physical nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce738359-d717-4865-84d1-92cf5c7d73bb",
   "metadata": {},
   "source": [
    "Closing your cluster will release the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120b21f-deb6-4af0-9020-07512b634f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a14c6c-a0ad-45cd-8ac7-2a9f9cadc5a6",
   "metadata": {},
   "source": [
    "Make sure to stop your notebook kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
